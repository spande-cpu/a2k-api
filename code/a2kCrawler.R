# Forum Scraper for https://able2know.org/forum/philosophy/
# Required
library(tidyverse)
library(rvest)

# Get topics, urls and metadata
get_info <- function(base_url) {
  base_html <- read_html(base_url)
  ## Get thread index data
  Url <- base_html %>%
    html_nodes(".gridItem") %>%
    html_nodes(".title") %>%
    html_node("a") %>%
    html_attr("href")
  
  Titles <- base_html %>%
    html_nodes(".gridItem") %>%
    html_nodes(".title") %>%
    html_text2()
  
  Score <- base_html %>%
    html_nodes(".gridItem") %>%
    html_nodes(".postScore") %>%
    html_text2() %>%
    as.numeric()
  ## Get Meta-Data
  meta_data <- base_html %>%
    html_nodes(".gridItem") %>%
    html_nodes(".topicMeta ") %>%
    html_text2() %>%
    as_tibble() %>%
    separate(
      "value",
      c("Topics", "Creator", "Created", "Replies", "Views", "LastReply"),
      sep = "\n"
    ) %>%
    mutate(
      Topics = str_remove(Topics, "Forums: "),
      Creator = str_remove(Creator, "Question by |Discussion by "),
      Created = lubridate::parse_date_time(str_remove(Created, "Posted "), "%m/%d/%y %I:%M %p"),
      Replies = as.numeric(str_remove_all(str_remove(Replies, "Replies: "), ",")),
      Views = as.numeric(str_remove_all(str_remove(Views, "Views: "), ","))
    )
  
  cbind(Url, Titles, Score, meta_data)
}

# Get comment-level data for each Title
get_discussion <- function(thread_url) {
  thread_url %>%
    read_html() %>%
    html_nodes(".expandedPostBody") %>%
    html_text2() %>%
    str_squish() %>%
    str_replace_all("\"", "\'")
}
get_users <- function(thread_url) {
  thread_url %>%
    read_html() %>%
    html_nodes(".user") %>%
    html_text2()
}
get_dates <- function(thread_url) {
  thread_url %>%
    read_html() %>%
    html_nodes(".date.smalltxt") %>%
    html_text2() %>%
    lubridate::parse_date_time(., "%a %d %b, %Y %I:%M %p")
}

# Crawler
scrape_a2k <- function(page_url = "https://able2know.org/forum/philosophy/", n_pages = 20) {
  seed_url <- page_url
  seed_html <- read_html(seed_url)
  # Total No. of Pages in the Forum
  total_pages <- seed_html %>%
    html_nodes(".box.pagination") %>%
    html_text2() %>%
    str_replace(",", "") %>%
    str_extract_all(., "[0-9]+") %>%
    unlist() %>%
    as.numeric() %>%
    max(na.rm = T)
  # No, of Pages to Scrape
  if (n_pages == "all" & total_pages > 1) {
    crawler_depth <- 2:total_pages
  } else if (!is.numeric(n_pages) & n_pages != "all") {
    print(
      "Error: Please supply the number of pages to scrape or set n_pages = 'all' to scrape all pages in the forum (this may take some time)."
    )
  } else if (n_pages > total_pages | n_pages == 1 | n_pages == 0 | n_pages < 0) {
    crawler_depth <- 1
  } else {
    crawler_depth <- 2:n_pages
  }
  # Page urls to scrape content from
  if (isTRUE(crawler_depth == 1) | !is.numeric(crawler_depth) | isTRUE(is.infinite(crawler_depth))) {
    page_urls <- c(seed_url)
  } else {
    page_urls <- c(
      seed_url,
      paste0(seed_url, "page-", crawler_depth)
    )[crawler_depth] %>% .[complete.cases(.)]
  }
  
  master <- purrr::map_dfr(page_urls, get_info)
  
  master %>%
    mutate(comments = Url %>% purrr::map(get_discussion),
           commentedBy = Url %>% purrr::map(get_users),
           commentedAt = Url %>% purrr::map(get_dates))
}

# Get Data: Uncomment to run.
# data <- scrape_a2k(n_pages = 100)

